{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is created to simulate a task CDC (Mysql) - Kafka Topic - Spark Structured Stream data flow and data aggregation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Needed Modules and Initiation of Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding folder path to SYS to gather dat module into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql_analyzer\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from mysql.connector.errors import Error\n",
    "# Dat is a module consisting of data transformation methods written by myself\n",
    "import dat\n",
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "from json import dumps\n",
    "from json import loads\n",
    "import time\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer, TopicPartition, KafkaConsumer\n",
    "import xlsxwriter\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session & Gathering Needed Pyspark Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.1.2'\n",
    "import findspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType, StructField\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session with gathering stream package and setting up some configs around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master('local[*]')\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .appName(\"myAppName\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating mysql_analyzer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_works = mysql_analyzer.mysql_profiler('localhost',os.environ['MYSQLSERVER_USER'],\n",
    "                os.environ['MYSQLSERVER_PASS'],'sakila')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created CDC table for target mysql table (users_change_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE IF NOT EXISTS \\\n",
    "sakila.users_change_events (log_id BIGINT AUTO_INCREMENT,\\\n",
    "  event_type      TEXT,\\\n",
    "  event_timestamp TIMESTAMP,\\\n",
    "  user_id         INT,\\\n",
    "  user_name       TEXT,\\\n",
    "  user_email      TEXT,\\\n",
    "  PRIMARY KEY (log_id))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created trigger that insert CDC transactions into CDC table created on previous step (for insert events on customer table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TRIGGER IF NOT EXISTS sakila.user_insert_capture AFTER INSERT ON sakila.customer FOR EACH ROW \\\n",
    "  BEGIN INSERT INTO sakila.users_change_events \\\n",
    "  (event_type, \\\n",
    "   event_timestamp, \\\n",
    "   user_id, \\\n",
    "   user_name, \\\n",
    "   user_email) \\\n",
    " VALUES ( \\\n",
    "   'INSERT', \\\n",
    "   now(), \\\n",
    "   user_id, \\\n",
    "   user_name, \\\n",
    "   user_email); \\\n",
    "  END;\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created lastest cdc timestamp holding table \n",
    "### We are reading cdc last timestamp from previous data write action and saving into below created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE IF NOT EXISTS \\\n",
    "sakila.latest_cdc_timestamp (log_id BIGINT AUTO_INCREMENT,\\\n",
    "  event_type      TEXT,\\\n",
    "  event_timestamp TIMESTAMP,\\\n",
    "  PRIMARY KEY (log_id))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka script to write data from mysql to kafka topic by catching CDC on mysql table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "# Inserting into latest_cdc_timestamp table, latest CDC timestamp from CDC table to be able to detect if any new CDC occurred\n",
    "mysql_works.multiple_dataset_apply_mysql_insert(f\"INSERT INTO sakila.latest_cdc_timestamp \\\n",
    "                                                (event_type,event_timestamp) \\\n",
    "                                                VALUES ('{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_type FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}','{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_timestamp FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}')\")\n",
    "\n",
    "# Creating fake records to create data flow to MYSQL db\n",
    "mysql_works.fake_record_creator_sakila()\n",
    "\n",
    "# Getting CDC timestamps from related table to use on below if statement\n",
    "latest_saved_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                      FROM latest_cdc_timestamp')[0][0]\n",
    "\n",
    "latest_real_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                     FROM users_change_events')[0][0]\n",
    "\n",
    "# If CDC occurred instantiating KafkaProducre class and saving data into Kafka topic\n",
    "if latest_saved_cdc_log < latest_real_cdc_log:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "        sasl_mechanism='SCRAM-SHA-256',\n",
    "        security_protocol='SASL_SSL',\n",
    "        sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "        sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "        value_serializer = lambda m : dumps(m, default=str).encode(\"utf-8\")\n",
    "        # api_version_auto_timeout_ms=100000,    \n",
    "    )\n",
    "\n",
    "    for record in mysql_works.multiple_dataset_apply_mysql_query(f'SELECT * FROM customer WHERE last_update > \"{latest_saved_cdc_log}\"'):\n",
    "        data_dict = {\"customer_id\" : record[0],\"store_id\" : record[1],\"first_name\" : record[2] \\\n",
    "                     ,\"first_name\" : record[3],\"email\" : record[4],\"address_id\" : record[5] \\\n",
    "                      ,  \"last_update\" : record[8]}\n",
    "        producer.send(\"mysql_write\",data_dict) \n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from kafka topic by readstream method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\",'settled-terrapin-12518-eu2-kafka.upstash.io:9092')\\\n",
    "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\\\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "  .option(\"kafka.sasl.jaas.config\",\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCSqaSFgt-fI-8JyIV50sk_wCOG7dRr8LsY\" password=\"Y2FhZGE3ZWQtYzQxOC00ZTdiLWJlZjUtOGRhMjJjN2YwZjU1\";\"\"\")\\\n",
    "  .option(\"subscribe\", \"mysql_write\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"checkpointLocation\", \"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks/csv_sink_2\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Source Schema for read operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address_id\", StringType(), True),\n",
    "    StructField(\"latest_update\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing JSON formatted string into tabular format for further aggregation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting binary to string\n",
    "csvDF = csvDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"CAST(timestamp AS Timestamp)\")\n",
    "# Exploding JSON formatted string column as defined on our schema\n",
    "csvDF = csvDF.withColumn(\"user_array\", F.from_json(\"value\",df_schema))\n",
    "# Creating needed columns and filling them with getItem method from json string\n",
    "csvDF = csvDF.withColumn(\"customer_id\", csvDF.user_array.getItem(\"customer_id\")).withColumn(\"store_id\", csvDF.user_array.getItem(\"store_id\")).withColumn(\"first_name\", csvDF.user_array.getItem(\"first_name\")).withColumn(\"latest_update\", csvDF.user_array.getItem(\"latest_update\"))\n",
    "# Selecting ultimate needed columns\n",
    "csvDF = csvDF.select(\"key\",\"customer_id\", \"store_id\", \"first_name\",\"latest_update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking latest format of schema before writestream operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- latest_update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation on stream data (Counting store_id customer qty. with in 30 seconds of tumbling windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = csvDF \\\n",
    "    .withWatermark(\"latest_update\", \"1 minutes\") \\\n",
    "    .groupBy(F.window(csvDF.latest_update, \"30 seconds\"),\n",
    "        csvDF.store_id) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating writestream query to query it by spark sql on next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x1c2a5363950>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"store_id_agg\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|window|store_id|count|\n",
      "+------+--------+-----+\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from store_id_agg\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
