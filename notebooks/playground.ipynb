{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import os\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.1.2'\n",
    "import findspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "# sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType, StructField\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark session with gathering stream package and some configs around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master('local[*]')\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .appName(\"myAppName\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changed below config approach on later works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kafka configs\n",
    "# kafka_source_config = {\n",
    "#     # \"kafka.sasl.jaas.config\": jaas_config,\n",
    "#     \"kafka.bootstrap.servers\" : \"settled-terrapin-12518-eu2-kafka.upstash.io:9092\",\n",
    "#     \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
    "#     \"kafka.security.protocol\" : \"SASL_SSL\",\n",
    "#     \"subscribe\": \"sec_topic\",\n",
    "#     \"startingOffsets\": \"earliest\",\n",
    "#     \"failOnDataLoss\": \"false\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Needed if sink is Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_sink_config = {\n",
    "#     \"kafka.sasl.jaas.config\": jaas_config,\n",
    "#     \"kafka.bootstrap.servers\": 'settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "#     \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
    "#     \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "#     \"topic\": \"sink\",\n",
    "#     \"checkpointLocation\" : \"./checkpoint.txt\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read stream from Uptash kafka server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\",'settled-terrapin-12518-eu2-kafka.upstash.io:9092')\\\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "    .option(\"kafka.sasl.jaas.config\",\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCSqaSFgt-fI-8JyIV50sk_wCOG7dRr8LsY\" password=\"Y2FhZGE3ZWQtYzQxOC00ZTdiLWJlZjUtOGRhMjJjN2YwZjU1\";\"\"\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"subscribe\", \"mysql\")\\\n",
    "    .load()\n",
    "    # .selectExpr(\"CAST(value AS STRING)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read\\\n",
    "#     .format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\",'settled-terrapin-12518-eu2-kafka.upstash.io:9092')\\\n",
    "#     .option(\"subscribe\",\"sec_topic\")\\\n",
    "#     .option(\"startingOffsets\",\"earliest\")\\\n",
    "#     .load()\n",
    "\n",
    "## Read Stream\n",
    "# streaming_df = spark\\\n",
    "#     .readStream\\\n",
    "#     .format(\"kafka\")\\\n",
    "#     .options(**kafka_source_config)\\\n",
    "#     .option(\"kafka.sasl.jaas.config\",\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc\" password=\"N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix\";\"\"\")\\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source schema on JSON explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source Schema\n",
    "df_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address_id\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting and exploding value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string\n",
    "value_df = streaming_df.select(F.from_json(F.col(\"value\").cast(\"string\"),df_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- store_id: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- address_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_expanded_df = json_df.withColumn(\"name\", F.from_json(json_df[\"value\"], df_schema)).select(\"value.*\")\n",
    "explode_df = value_df.selectExpr(\"value.store_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema control for dataframe to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing stream to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate running word count\n",
    "store_count = explode_df.groupBy(\"store_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode_df.createOrReplaceTempView(\"updates\")\n",
    "# spark.sql(\"select store_id, count(*) from updates\")  # returns another streaming DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = store_count \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "\n",
    "# query = store_count \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"append\") \\\n",
    "#     .outputMode(\"append\").option(\"path\", \"C:/Users/Lenovo/Desktop/spark/New folder/spark_stream_job_6/stream_output\") \\\n",
    "#     .option(\"checkpointLocation\", \"C:/Users/Lenovo/Desktop/spark/New folder/spark_stream_job_6/checkpoint\") \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowedCounts = streaming_df \\\n",
    "#     .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "#     .groupBy(\n",
    "#         window(streaming_df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "#         words.word) \\\n",
    "#     .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 595c0039-4530-4b47-a3b9-fde7f1043d9e, runId = e82225a6-6de0-4f43-af6b-71008a64d899] terminated with exception: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1719501080816, tries=1, nextAllowedTryMs=1719501080931) timed out at 1719501080831 after 1 attempt(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m baboli \u001b[38;5;241m=\u001b[39m store_count\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \\\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettled-terrapin-12518-eu2-kafka.upstash.io:9092\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.sasl.mechanism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCRAM-SHA-256\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Lenovo/Desktop/spark/New folder/spark_stream_job_11/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# writeStream\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     .format(\"kafka\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#     .option(\"topic\", \"updates\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     .start()\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mbaboli\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\streaming\\query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 595c0039-4530-4b47-a3b9-fde7f1043d9e, runId = e82225a6-6de0-4f43-af6b-71008a64d899] terminated with exception: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeTopics, deadlineMs=1719501080816, tries=1, nextAllowedTryMs=1719501080931) timed out at 1719501080831 after 1 attempt(s)"
     ]
    }
   ],
   "source": [
    "baboli = store_count.writeStream.format(\"kafka\").outputMode(\"complete\")  \\\n",
    "    .option(\"kafka.bootstrap.servers\",'settled-terrapin-12518-eu2-kafka.upstash.io:9092')\\\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "    .option(\"kafka.sasl.jaas.config\",\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCSqaSFgt-fI-8JyIV50sk_wCOG7dRr8LsY\" password=\"Y2FhZGE3ZWQtYzQxOC00ZTdiLWJlZjUtOGRhMjJjN2YwZjU1\";\"\"\")\\\n",
    "    .option(\"topic\", \"mysql_write\") \\\n",
    "    .option(\"checkpointLocation\", \"C:/Users/Lenovo/Desktop/spark/New folder/spark_stream_job_11/checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "# writeStream\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "#     .option(\"topic\", \"updates\")\n",
    "#     .start()\n",
    "baboli.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only needed to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_writer.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uptash`s own method to read from topic could be used as alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message: b'{\"name\": \"Thomas Lopez\"}'\n",
      "Received message: b'{\"name\": \"Sharon Rivera\"}'\n",
      "Received message: b'{\"name\": \"Jonathan Rodriguez\"}'\n",
      "Received message: b'{\"name\": \"Melissa Hammond\"}'\n",
      "Received message: b'{\"name\": \"Daniel Frederick\"}'\n",
      "Received message: b'{\"name\": \"Henry Herman\"}'\n",
      "Received message: b'{\"name\": \"Sherry Cantrell\"}'\n",
      "Received message: b'{\"name\": \"Tommy Smith\"}'\n",
      "Received message: b'{\"name\": \"Tammy Nguyen\"}'\n",
      "Received message: b'{\"name\": \"Edward Estrada\"}'\n",
      "Received message: b'{\"name\": \"Dustin Thomas\"}'\n",
      "Received message: b'{\"name\": \"Katrina Lee\"}'\n",
      "Received message: b'{\"name\": \"Gary Hill\"}'\n",
      "Received message: b'{\"name\": \"Kristin Gonzalez\"}'\n",
      "Received message: b'{\"name\": \"Joel Jackson\"}'\n",
      "Received message: b'{\"name\": \"Timothy Fernandez\"}'\n",
      "Received message: b'{\"name\": \"Marcus Diaz\"}'\n",
      "Received message: b'{\"name\": \"Stephanie Green\"}'\n",
      "Received message: b'{\"name\": \"Brianna Jones\"}'\n"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'dingilaz',\n",
    "    bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "    sasl_mechanism='SCRAM-SHA-256',\n",
    "    security_protocol='SASL_SSL',\n",
    "    sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "    sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "    group_id='YOUR_CONSUMER_GROUP',\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        print(f\"Received message: {message.value}\")\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
