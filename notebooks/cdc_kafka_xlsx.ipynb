{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is created to simulate a task CDC (Mysql) - Kafka Topic - local csv file - Spark Structured Stream data flow and data aggregation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Needed Modules and Initiation of Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding folder path to SYS to gather dat module into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "# mysql_analyzer is a module consisting of data transformation methods written by myself\n",
    "import mysql_analyzer\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from mysql.connector.errors import Error\n",
    "# Dat is a module consisting of data transformation methods written by myself\n",
    "import dat\n",
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "from json import dumps\n",
    "from json import loads\n",
    "import time\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer, TopicPartition, KafkaConsumer\n",
    "import xlsxwriter\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session & Gathering Needed Pyspark Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.1.2'\n",
    "import findspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType, StructField\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session with gathering stream package and setting up some configs around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master('local[*]')\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .appName(\"myAppName\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating mysql_analyzer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_works = mysql_analyzer.mysql_profiler('localhost',os.environ['MYSQLSERVER_USER'],\n",
    "                os.environ['MYSQLSERVER_PASS'],'sakila')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created CDC table for target mysql table (users_change_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE IF NOT EXISTS \\\n",
    "sakila.users_change_events (log_id BIGINT AUTO_INCREMENT,\\\n",
    "  event_type      TEXT,\\\n",
    "  event_timestamp TIMESTAMP,\\\n",
    "  user_id         INT,\\\n",
    "  user_name       TEXT,\\\n",
    "  user_email      TEXT,\\\n",
    "  PRIMARY KEY (log_id))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created trigger that insert CDC transactions into CDC table created on previous step (for insert events on customer table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TRIGGER IF NOT EXISTS sakila.user_insert_capture AFTER INSERT ON sakila.customer FOR EACH ROW \\\n",
    "  BEGIN INSERT INTO sakila.users_change_events \\\n",
    "  (event_type, \\\n",
    "   event_timestamp, \\\n",
    "   user_id, \\\n",
    "   user_name, \\\n",
    "   user_email) \\\n",
    " VALUES ( \\\n",
    "   'INSERT', \\\n",
    "   now(), \\\n",
    "   user_id, \\\n",
    "   user_name, \\\n",
    "   user_email); \\\n",
    "  END;\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created lastest cdc timestamp holding table \n",
    "### We are reading cdc last timestamp from previous data write action and saving into below created table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO STATEMENT EXECUTED'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE IF NOT EXISTS \\\n",
    "sakila.latest_cdc_timestamp (log_id BIGINT AUTO_INCREMENT,\\\n",
    "  event_type      TEXT,\\\n",
    "  event_timestamp TIMESTAMP,\\\n",
    "  PRIMARY KEY (log_id))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka script to write data from mysql to kafka topic by catching CDC on mysql table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n"
     ]
    }
   ],
   "source": [
    "# Inserting into latest_cdc_timestamp table, latest CDC timestamp from CDC table to be able to detect if any new CDC occurred\n",
    "mysql_works.multiple_dataset_apply_mysql_insert(f\"INSERT INTO sakila.latest_cdc_timestamp \\\n",
    "                                                (event_type,event_timestamp) \\\n",
    "                                                VALUES ('{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_type FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}','{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_timestamp FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}')\")\n",
    "\n",
    "# Creating fake records to create data flow to MYSQL db\n",
    "mysql_works.fake_record_creator_sakila()\n",
    "\n",
    "# Getting CDC timestamps from related table to use on below if statement\n",
    "latest_saved_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                      FROM latest_cdc_timestamp')[0][0]\n",
    "\n",
    "latest_real_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                     FROM users_change_events')[0][0]\n",
    "\n",
    "# If CDC occurred instantiating KafkaProducre class and saving data into Kafka topic\n",
    "if latest_saved_cdc_log < latest_real_cdc_log:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "        sasl_mechanism='SCRAM-SHA-256',\n",
    "        security_protocol='SASL_SSL',\n",
    "        sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "        sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "        value_serializer = lambda m : dumps(m, default=str).encode(\"utf-8\")\n",
    "        # api_version_auto_timeout_ms=100000,    \n",
    "    )\n",
    "\n",
    "    for record in mysql_works.multiple_dataset_apply_mysql_query(f'SELECT * FROM customer WHERE last_update > \"{latest_saved_cdc_log}\"'):\n",
    "        data_dict = {\"customer_id\" : record[0],\"store_id\" : record[1],\"first_name\" : record[2] \\\n",
    "                     ,\"first_name\" : record[3],\"email\" : record[4],\"address_id\" : record[5] \\\n",
    "                      ,  \"last_update\" : record[8]}\n",
    "        producer.send(\"mysql_write\",data_dict) \n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved data from kafka topic into a local xlsx file (Append method use to get only new records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'mysql_write'\n",
    "tp = TopicPartition(topic,0)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "    sasl_mechanism='SCRAM-SHA-256',\n",
    "    security_protocol='SASL_SSL',\n",
    "    sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "    sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "    # group_id='bacak',\n",
    "    auto_offset_reset='earliest',\n",
    "    # max_poll_interval_ms=30,\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Configs to get latest offset\n",
    "consumer.assign([tp])\n",
    "\n",
    "# obtain the last offset value\n",
    "consumer.seek_to_end(tp)\n",
    "lastOffset = consumer.position(tp)\n",
    "\n",
    "# Configs to get latest offset\n",
    "consumer.seek_to_beginning(tp)   \n",
    "\n",
    "# Saved topic messages into dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Stopped reading at latest offset\n",
    "for message in consumer:\n",
    "    if message.offset == lastOffset - 1:\n",
    "        break\n",
    "    else:\n",
    "        message = message.value;\n",
    "        df_raw = pd.json_normalize(message, max_level=0)\n",
    "        df = pd.concat([df, df_raw])\n",
    "\n",
    "\n",
    "df.to_csv('stream_df.csv', index=False) \n",
    "\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved csv as spark stream source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source Schema\n",
    "df_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address_id\", StringType(), True),\n",
    "    StructField(\"latest_update\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(df_schema) \\\n",
    "    .csv(\"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: string (nullable = true)\n",
      " |-- latest_update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation on stream data (Counting store_id customer qty. with in 30 seconds of tumbling windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = csvDF \\\n",
    "    .withWatermark(\"latest_update\", \"1 minutes\") \\\n",
    "    .groupBy(F.window(csvDF.latest_update, \"30 seconds\"),\n",
    "        csvDF.store_id) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedCounts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating writestream query to query it by spark sql on next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x2b51d3bad50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"store_id_agg\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|              window|store_id|count|\n",
      "+--------------------+--------+-----+\n",
      "|{2024-06-28 15:11...|       1|    5|\n",
      "|{2024-06-28 15:15...|       1|    4|\n",
      "|{2024-06-28 15:38...|       1|    4|\n",
      "|{2024-06-28 15:39...|       1|    4|\n",
      "|{2024-06-28 15:19...|       1|    5|\n",
      "|{2024-06-28 16:10...|       2|    2|\n",
      "|{2024-06-28 15:43...|       1|    5|\n",
      "|{2024-07-01 13:35...|       2|    2|\n",
      "|{2024-06-28 14:34...|       1|    2|\n",
      "|{2024-07-01 12:53...|       1|    2|\n",
      "|{2024-07-01 12:48...|       1|    1|\n",
      "|{2024-07-01 13:24...|       2|    1|\n",
      "|{2024-07-01 12:13...|       1|    2|\n",
      "|{2024-07-01 12:59...|       2|    3|\n",
      "|{2024-07-01 13:38...|       1|    2|\n",
      "|{2024-07-01 13:51...|       2|    2|\n",
      "|{2024-06-28 15:20...|       1|    4|\n",
      "|{2024-06-28 15:19...|       1|    5|\n",
      "|{2024-07-01 12:50...|       2|    2|\n",
      "|{2024-06-28 15:36...|       1|    4|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from store_id_agg\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
